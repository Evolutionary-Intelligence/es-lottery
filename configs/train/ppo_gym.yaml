train_config:
  env_name: "Pendulum-v1"
  model_config:
    hidden_layers: 2
    hidden_dims: 32

  ppo_config:
    env_name: "Pendulum-v1"
    num_train_steps: 1750000
    evaluate_every_epochs: 1000
    num_test_rollouts: 164
    num_train_envs: 32  # Number of parallel env workers
    max_grad_norm: 1.0  # Global norm to clip gradients by
    gamma: 0.97  # Discount factor
    n_steps: 16 # "GAE n-steps"
    n_minibatch: 8 # "Number of PPO minibatches"
    lr_begin: 3e-04  # Start PPO learning rate
    lr_end: 1e-04 #  End PPO learning rate
    lr_warmup: 0.05 # Prop epochs until warmup is completed 
    epoch_ppo: 4  # "Number of PPO epochs on a single batch"
    clip_eps: 0.25 # "Clipping range"
    gae_lambda: 0.95 # "GAE lambda"
    entropy_coeff: 0.005 # "Entropy loss coefficient"
    critic_coeff: 0.5  # "Value loss coefficient"

  num_imp_iters: 20
  imp_config:
    baseline_name: "final-ticket"
    prune_ratio: 0.8
    modules_not_to_prune:
      - params/CriticMLP_0/critic_fc_1/kernel
      - params/CriticMLP_0/critic_fc_2/kernel
      - params/CriticMLP_0/critic_fc_v/kernel
  network_to_prune: "final"
  fixed_seed: false

log_config:
  time_to_track: ["imp_iter", "num_steps"]
  what_to_track: 
    - "final_perf"
    - "best_perf"
    - "test_perf"
    - "train_perf"
    - "sparsity"
    - "trainable_params"
    - "pruned_params"
    - "prunable_params"
    - "prune_threshold"
  what_to_print:
    - "final_perf"
    - "best_perf"
    - "test_perf"
    - "sparsity"
  verbose: true
  print_every_k_updates: 1
  overwrite: 1
  model_type: "jax"
  use_wandb: true
  wandb_config:
    project: es-lottery
    group: pendulum-pgpe-sigma-track
    name: seed0

device_config:
  device_type: "gpu"
  num_devices: 1